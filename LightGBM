import os
import pandas as pd
import numpy as np
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score
import matplotlib.pyplot as plt


# After extracting the feature indices and getting the extracted_features.csv

def load_and_merge_data(feature_file, age_file):
    """
    Load features and age CSVs, merge on Patient_ID, and return a DataFrame.
    """
    try:
        features_df = pd.read_csv(feature_file)
        if 'ID' not in features_df.columns:
            raise ValueError("ID column not found in extracted_features.csv")
        
        age_df = pd.read_csv(age_file)
        if 'ID' not in age_df.columns or 'Age' not in age_df.columns:
            raise ValueError("ID or Age column not found in oasis_cross-sectional-5708aa0a98d82080.csv")
        
        df = pd.merge(features_df, age_df[['ID', 'Age']], on='ID', how='inner')
        
        if df.empty:
            raise ValueError("No matching Patient_IDs found between feature and age CSVs")
        
        print(f"Merged data for {len(df)} patients.")
        return df
    except Exception as e:
        print(f"Error loading or merging data: {e}")
        return None



def perform_lgbm_regression(df):
    """
    Perform LightGBM regression with enhanced training and calculate brain age gap.
    Returns model, predictions, feature importances, and brain age gap.
    """
    X = df.drop(columns=['ID', 'Age'])
    y = df['Age']
    feature_names = X.columns
    
    # Normalize features (optional for LightGBM, included for consistency)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    # Define LightGBM model and hyperparameter grid, finetune needed
    lgbm = LGBMRegressor(random_state=42, verbosity=-1)
    param_grid = {
        'n_estimators': [100, 200, 500], # can be adjusted into a higher level
        'learning_rate': [0.01, 0.05, 0.1], # can be adjusted into a lower level
        'max_depth': [3, 5, 7, -1],  # -1 means no limit, expand it
        'num_leaves': [20, 31, 50], # can be lower, higher level will cause overfitting
        'min_data_in_leaf': [5, 10, 20], # can be higher, stablize small dataset
        'feature_fraction': [0.5, 0.8, 1.0] # can be lower, for reducing the noise
    }
    
    # Perform grid search with cross-validation
    grid_search = GridSearchCV(lgbm, param_grid, cv=10, scoring='neg_mean_absolute_error', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    
    # Best model
    model = grid_search.best_estimator_
    print(f"Best LightGBM parameters: {grid_search.best_params_}")
    
    # Cross-validated performance
    mae_scores = -cross_val_score(model, X_scaled, y, cv=10, scoring='neg_mean_absolute_error')
    print(f"Cross-validated MAE: {mae_scores.mean():.2f} ± {mae_scores.std():.2f} years")
    
    # Predict brain age
    y_pred = model.predict(X_test)
    
    # Calculate brain age gap
    brain_age_gap = y_pred - y_test
    
    # Evaluate on test set
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"Test Set Mean Absolute Error: {mae:.2f} years")
    print(f"Test Set R² Score: {r2:.2f}")
    
    # Feature importance
    importances = model.feature_importances_ / model.feature_importances_.sum()  # Normalize
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    print("\nFeature Importance:")
    print(feature_importance_df)
    
    return model, X_test, y_test, y_pred, brain_age_gap, df.loc[y_test.index, 'ID'], feature_importance_df



def visualize_results(y_test, y_pred, brain_age_gap, feature_importance_df):
    """
    Visualize predicted vs actual age, brain age gap distribution, and feature importance.
    """
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual Age')
    plt.ylabel('Predicted Age')
    plt.title('Predicted vs Actual Brain Age (LightGBM)')
    
    plt.subplot(1, 3, 2)
    plt.hist(brain_age_gap, bins=20, edgecolor='black')
    plt.xlabel('Brain Age Gap (Predicted - Actual)')
    plt.ylabel('Frequency')
    plt.title('Brain Age Gap Distribution (LightGBM)')
    
    plt.subplot(1, 3, 3)
    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
    plt.xlabel('Importance')
    plt.title('Feature Importance')
    
    plt.tight_layout()
    plt.show()


# Now do it !!!

if __name__ == "__main__":
    parent_dir = input("/Users/wjw_4/Documents/Helsa/extract")
    
    feature_file = os.path.join(parent_dir, 'extracted_features.csv')
    age_file = os.path.join(parent_dir, 'oasis_cross-sectional-5708aa0a98d82080.csv')
    
    print("Loading and merging data...")
    df = load_and_merge_data(feature_file, age_file)
    
    if df is None or df.empty:
        print("Failed to load data. Please check file paths and formats.")
    else:
        print("Data preview:")
        print(df.head())
        
        print("\nTraining LightGBM model...")
        model, X_test, y_test, y_pred, brain_age_gap, test_patient_ids, feature_importance_df = perform_lgbm_regression(df)
        
        print("\nVisualizing results...")
        visualize_results(y_test, y_pred, brain_age_gap, feature_importance_df)
        
        results = pd.DataFrame({
            'ID': test_patient_ids,
            'Actual_Age': y_test,
            'Predicted_Age': y_pred,
            'Brain_Age_Gap': brain_age_gap
        })
        results.to_csv(os.path.join(parent_dir, 'RE_lgbm_results.csv'), index=False)
        print(f"\nResults saved to {os.path.join(parent_dir, 'RE_lgbm_results.csv')}")







# Can consider refine a bit.


def perform_lgbm_regression(df, importance_threshold=0.05):
    """
    Perform LightGBM regression with optimized training and calculate brain age gap.
    Returns model, predictions, feature importances, and brain age gap.
    """
    X = df.drop(columns=['ID', 'Age'])
    y = df['Age']
    feature_names = X.columns
    
    # Normalize features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)
    
    # Define LightGBM model and reduced hyperparameter grid
    lgbm = LGBMRegressor(random_state=42, verbosity=-1)
    param_grid = {
        'n_estimators': [100, 200, 500],
        'learning_rate': [0.01, 0.05, 0.1],
        'max_depth': [3, 5, 7],
        'num_leaves': [15, 31],
        'min_data_in_leaf': [5, 10, 20],
        'feature_fraction': [0.6, 0.8],
        'lambda_l1': [0, 0.1],
        'lambda_l2': [0, 0.1]
    }
    
    # Perform grid search with cross-validation
    grid_search = GridSearchCV(lgbm, param_grid, cv=10, scoring='neg_mean_absolute_error', n_jobs=-1)
    grid_search.fit(X_train, y_train)
    
    # Best model
    model = grid_search.best_estimator_
    print(f"Best LightGBM parameters: {grid_search.best_params_}")
    
    # Retrain with early stopping
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='mae', callbacks=[lightgbm.early_stopping(stopping_rounds=10, verbose=False)])
    
    # Cross-validated performance
    mae_scores = -cross_val_score(model, X_scaled, y, cv=10, scoring='neg_mean_absolute_error')
    print(f"Cross-validated MAE: {mae_scores.mean():.2f} ± {mae_scores.std():.2f} years")
    
    # Predict brain age
    y_pred = model.predict(X_test)
    
    # Calculate brain age gap
    brain_age_gap = y_pred - y_test
    
    # Evaluate on test set
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"Test Set Mean Absolute Error: {mae:.2f} years")
    print(f"Test Set R² Score: {r2:.2f}")
    
    # Feature importance
    importances = model.feature_importances_ / model.feature_importances_.sum()
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False)
    print("\nFeature Importance:")
    print(feature_importance_df)
    
    # Suggest features to keep
    important_features = feature_importance_df[feature_importance_df['Importance'] >= importance_threshold]['Feature'].tolist()
    print(f"\nSuggested features to keep (importance >= {importance_threshold}):")
    print(important_features)
    
    return model, X_test, y_test, y_pred, brain_age_gap, df.loc[y_test.index, 'ID'], feature_importance_df

def visualize_results(y_test, y_pred, brain_age_gap, feature_importance_df):
    """
    Visualize predicted vs actual age, brain age gap distribution, and feature importance.
    """
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.scatter(y_test, y_pred, alpha=0.5)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.xlabel('Actual Age')
    plt.ylabel('Predicted Age')
    plt.title('Predicted vs Actual Brain Age (LightGBM)')
    
    plt.subplot(1, 3, 2)
    plt.hist(brain_age_gap, bins=20, edgecolor='black')
    plt.xlabel('Brain Age Gap (Predicted - Actual)')
    plt.ylabel('Frequency')
    plt.title('Brain Age Gap Distribution (LightGBM)')
    
    plt.subplot(1, 3, 3)
    plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])
    plt.xlabel('Importance')
    plt.title('Feature Importance')
    
    plt.tight_layout()
    plt.show()



if __name__ == "__main__":
    import lightgbm  # Import here to use early_stopping callback
    parent_dir = input("Enter the parent directory path (containing extract folder and CSVs),or press enter: ")
    
    feature_file = os.path.join(parent_dir, 'extracted_features.csv')
    age_file = os.path.join(parent_dir, 'oasis_cross-sectional-5708aa0a98d82080.csv')
    
    print("Loading and merging data...")
    df = load_and_merge_data(feature_file, age_file)
    
    if df is None or df.empty:
        print("Failed to load data. Please check file paths and formats.")
    else:
        print("Data preview:")
        print(df.head())
        
        print("\nTraining LightGBM model...")
        model, X_test, y_test, y_pred, brain_age_gap, test_patient_ids, feature_importance_df = perform_lgbm_regression(df)
        
        print("\nVisualizing results...")
        visualize_results(y_test, y_pred, brain_age_gap, feature_importance_df)
        
        results = pd.DataFrame({
            'ID': test_patient_ids,
            'Actual_Age': y_test,
            'Predicted_Age': y_pred,
            'Brain_Age_Gap': brain_age_gap
        })
        results.to_csv(os.path.join(parent_dir, 'NEW_RE_lgbm_results.csv'), index=False)
        print(f"\nResults saved to {os.path.join(parent_dir, 'NEW_RE_lgbm_results.csv')}")
